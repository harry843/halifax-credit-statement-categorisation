{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from PyPDF2 import PdfReader\n",
    "from os import listdir, getenv\n",
    "import re\n",
    "import openai\n",
    "from numpy import isnan\n",
    "from dotenv import load_dotenv\n",
    "from json import dump, load\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Statement Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the folder containing your halifax credit statements: C:\\Users\\kelle\\OneDrive\\Documents\\Finances\\Statements\\Credit\n"
     ]
    }
   ],
   "source": [
    "# Load in environmental variables and define paths to local files\n",
    "env_path = r\"openai.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "openai.api_key = getenv(\"OPENAI_ACCESS_TOKEN\")\n",
    "\n",
    "credit_statement_location = input(\"Enter the path to the folder containing your pdf Halifax credit statements:\")\n",
    "files = [i for i in listdir(credit_statement_location) if i[-4:]==\".pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils Functions\n",
    "def chunks(xs: list, n: int) -> Iterator[list]:\n",
    "    \"\"\" Splits input list into len(xs)/n lists of n elements\n",
    "    \"\"\"\n",
    "    n = max(1,n)\n",
    "    return (xs[i:i+n] for i in range(0, len(xs), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Checks\n",
    "def df_length_equals_pdf_length(df: pd.DataFrame, extracted_text: str):\n",
    "    \"\"\" Checks whether length of output dataframe equals the\n",
    "        length of the transactions table in the Pdf.\n",
    "    \"\"\"\n",
    "    pattern = r'(\\n\\d{2}\\s(?:JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER))'\n",
    "\n",
    "    matches = re.findall(pattern,extracted_text)\n",
    "    assert len(df) == len(matches), f\"df of length {len(df)} does not equal matches of length {len(matches)}\"\n",
    "    \n",
    "def impute_transaction_date(file_date: str, record: str):\n",
    "    file_month, file_year = file_date.split('-')\n",
    "    transaction_day, transaction_month = record.split(' ')[0:2]\n",
    "    if file_month == 'Jan' and transaction_month == 'DECEMBER':\n",
    "        return transaction_day + ' ' + transaction_month + ' ' + str(int(file_year) - 1)\n",
    "    return transaction_day + ' ' + transaction_month + ' ' + file_year\n",
    "        \n",
    "    \n",
    "def is_same_or_previous_month(parsed_date, file_date_string):\n",
    "    # Parse the date strings into datetime objects\n",
    "    file_date_obj = datetime.strptime(file_date_string, '%b-%y')\n",
    "\n",
    "    # Check if the date is the same or one month before the file_date\n",
    "    if parsed_date.month == file_date_obj.month and parsed_date.year == file_date_obj.year:\n",
    "        return True\n",
    "    elif parsed_date.month == (file_date_obj.month - 1) and parsed_date.year == file_date_obj.year:\n",
    "        return True\n",
    "    elif file_date_obj.month == 1:\n",
    "        parsed_date.month == file_date_obj.month or parsed_date.month == 12\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_valid_transaction_date(date: str, file_date: str):\n",
    "    parsed_date = pd.to_datetime(date, format=\"%d %B %y\")\n",
    "    month, year = file_date.split('-')\n",
    "    \n",
    "    assert parsed_date <= datetime.now(), f\"{file_date}: date extracted is in the future: {date}\"\n",
    "\n",
    "    # Check year is valid\n",
    "    if month == 'Jan' and parsed_date.month == 12:\n",
    "        assert str(parsed_date.year)[-2:] == str(int(year) - 1), f\"File year {year} does not match year parsed {parsed_date.year}\"\n",
    "    else:\n",
    "        assert str(parsed_date.year)[-2:] == year, f\"File year {year} does not match year parsed {parsed_date.year}\"\n",
    "    \n",
    "    # Check month is valid\n",
    "    assert is_same_or_previous_month(parsed_date, file_date), f\"File month {month} does not match month parsed {date.split(' ')[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to turn Halifax Credit Card statementscredit\n",
    "def credit_transactions_to_df(files: list) -> pd.DataFrame:\n",
    "    \"\"\" Reads in a list of Halifax Pdf Bank Statements, extracts transaction\n",
    "        data and transforms them into a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    count = 1\n",
    "    for f in files:\n",
    "        file_date = f[-10:-4]\n",
    "        year = f[-6:-4]\n",
    "        \n",
    "        if count == len(files):\n",
    "            print(f\"{file_date}\\nCompleted\")\n",
    "        else:\n",
    "            print(f\"{file_date}\",end=\" -> \")\n",
    "\n",
    "        reader = PdfReader(credit_statement_location + \"\\\\\" + f) \n",
    "\n",
    "        # getting a specific page from the pdf file (all my credit transactions are located on 1 page!)\n",
    "        page = reader.pages[2] \n",
    "\n",
    "        # extracting text from page \n",
    "        text = page.extract_text() \n",
    "\n",
    "        # Apply the regex pattern to extract the desired text\n",
    "        match = re.search(r'BALANCE FROM PREVIOUS STATEMENT([\\s\\S]*?)Customer Services:', text)\n",
    "\n",
    "        extracted_text = match.group(1) if match else None\n",
    "        assert extracted_text is not None, f\"Failed to capture any data for {f[-10:-4]} statement\"\n",
    "\n",
    "        pattern = r'(\\d{2}\\s(?:JANUARY|FEBRUARY|MARCH|APRIL|MAY|JUNE|JULY|AUGUST|SEPTEMBER|OCTOBER|NOVEMBER|DECEMBER))'\n",
    "        \n",
    "        records = [t.strip() for t in extracted_text.split('\\n') if re.findall(pattern,t)]\n",
    "\n",
    "        credit_transactions = []\n",
    "        for record in records:\n",
    "            checking_df = pd.DataFrame()\n",
    "            transaction_date = impute_transaction_date(file_date, record)\n",
    "            # Check transaction date extracted is valid\n",
    "            is_valid_transaction_date(transaction_date, file_date)\n",
    "            description = ' '.join(record.split(' ')[4:-2]).strip().upper()\n",
    "            if record.split(' ')[-1] == 'CR':\n",
    "                credit_amount = record.split(' ')[-2].replace(\",\",\"\")\n",
    "                debit_amount = None\n",
    "                credit_transactions.append({\"Transaction Date\": transaction_date, \"Transaction Description\":description, \"Debit Amount\":None, \"Credit Amount\":credit_amount})\n",
    "            else:\n",
    "                debit_amount = record.split(' ')[-1].replace(\",\",\"\")\n",
    "                credit_amount = None\n",
    "                credit_transactions.append({\"Transaction Date\": transaction_date, \"Transaction Description\":description, \"Debit Amount\":debit_amount, \"Credit Amount\":None})\n",
    "            part_df = pd.DataFrame(credit_transactions)\n",
    "        # Check output df length equals number of records in pdf\n",
    "        checking_df = pd.concat([checking_df,part_df])\n",
    "        df_length_equals_pdf_length(part_df, extracted_text)\n",
    "        df = pd.concat([df, part_df]).reset_index(drop=True)\n",
    "        df_length_equals_pdf_length(part_df, extracted_text)\n",
    "        # Increase count by 1 to track progress\n",
    "        count += 1\n",
    "\n",
    "    df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], format=\"%d %B %y\")\n",
    "    df['Debit Amount'] = df['Debit Amount'].astype('float')\n",
    "    df['Credit Amount'] = df['Credit Amount'].astype('float')\n",
    "    df['Card Used'] = 'Halifax Credit'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT functions\n",
    "def query_chatGPT(prompt: str) -> str:\n",
    "    \"\"\" Takes string prompt to submit as a query to ChatGPT 3.5 Turbo\n",
    "        Returns ChatGPT's response as a string\n",
    "    \"\"\"\n",
    "    completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":first_prompt.format(debit_categories, chunk)}\n",
    "    ])\n",
    "    response = completion.choices[0].message.content\n",
    "    \n",
    "    return response \n",
    "\n",
    "def extract_dict_from_chatGPT_response(response: str) -> dict:\n",
    "    \"\"\" Searches ChatGPT's string response for text within curly brackets\n",
    "        Returns all key:value pairs found as a dictionary\n",
    "    \"\"\"\n",
    "    # Captures all text within curly brackets (aka the output dictionary)\n",
    "    pattern = r'{(.*?)}'\n",
    "\n",
    "    # Captures only the key and value, within single quotation marks in the string\n",
    "    str_pattern = r\"'([^']+)':\\s+'([^']+)'\"\n",
    "\n",
    "    # Find all matches in the text\n",
    "    matches = re.findall(pattern, response, re.DOTALL)\n",
    "\n",
    "    # Extracted data inside and including the curly brackets\n",
    "    dictionary={}\n",
    "\n",
    "    for match in matches:\n",
    "        # Find all matches in the string\n",
    "        str_matches = re.findall(str_pattern, match)\n",
    "\n",
    "        # Iterate through the matches and store them in the dictionary\n",
    "        for str_match in str_matches:\n",
    "            key = str_match[0]\n",
    "            value = str_match[1]\n",
    "            dictionary[key] = value\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert PDF to DataFrame and define Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apr-20 -> Apr-21 -> Apr-22 -> Apr-23 -> Aug-20 -> Aug-21 -> Aug-22 -> Aug-23 -> Dec-19 -> "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcredit_transactions_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m debit_transactions \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCredit Amount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransaction Description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      5\u001b[0m debit_categories \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRent\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEating out\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransport\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroceries\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShopping\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHolidays\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntertainment\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPersonal Care\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneral\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mcredit_transactions_to_df\u001b[1;34m(files)\u001b[0m\n\u001b[0;32m     20\u001b[0m page \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mpages[\u001b[38;5;241m2\u001b[39m] \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# extracting text from page \u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Apply the regex pattern to extract the desired text\u001b[39;00m\n\u001b[0;32m     26\u001b[0m match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBALANCE FROM PREVIOUS STATEMENT([\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS]*?)Customer Services:\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_page.py:1612\u001b[0m, in \u001b[0;36mPageObject.extract_text\u001b[1;34m(self, Tj_sep, TJ_sep, orientations, space_width, *args)\u001b[0m\n\u001b[0;32m   1609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orientations, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m   1610\u001b[0m     orientations \u001b[38;5;241m=\u001b[39m (orientations,)\n\u001b[1;32m-> 1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morientations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCONTENTS\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_page.py:1205\u001b[0m, in \u001b[0;36mPageObject._extract_text\u001b[1;34m(self, obj, pdf, orientations, space_width, content_key)\u001b[0m\n\u001b[0;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Font\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resources_dict:\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m cast(DictionaryObject, resources_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Font\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m-> 1205\u001b[0m         cmaps[f] \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_char_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1206\u001b[0m cmap: Tuple[Union[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m]], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharmap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1208\u001b[0m     {},\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNotInitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1210\u001b[0m )  \u001b[38;5;66;03m# (encoding,CMAP,font_name)\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_cmap.py:22\u001b[0m, in \u001b[0;36mbuild_char_map\u001b[1;34m(font_name, space_width, obj)\u001b[0m\n\u001b[0;32m     20\u001b[0m space_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     21\u001b[0m encoding, space_code \u001b[38;5;241m=\u001b[39m parse_encoding(ft, space_code)\n\u001b[1;32m---> 22\u001b[0m map_dict, space_code, int_entry \u001b[38;5;241m=\u001b[39m \u001b[43mparse_to_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# encoding can be either a string for decode (on 1,2 or a variable number of bytes) of a char table (for 1 byte only for me)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# if empty string, it means it is than encoding field is not present and we have to select the good encoding from cmap input data\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_cmap.py:188\u001b[0m, in \u001b[0;36mparse_to_unicode\u001b[1;34m(ft, space_code)\u001b[0m\n\u001b[0;32m    186\u001b[0m cm \u001b[38;5;241m=\u001b[39m prepare_cm(ft)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m cm\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 188\u001b[0m     process_rg, process_char, multiline_rg \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_cm_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_rg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_char\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_rg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_entry\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a, value \u001b[38;5;129;01min\u001b[39;00m map_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_cmap.py:249\u001b[0m, in \u001b[0;36mprocess_cm_line\u001b[1;34m(l, process_rg, process_char, multiline_rg, map_dict, int_entry)\u001b[0m\n\u001b[0;32m    247\u001b[0m     process_char \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m process_rg:\n\u001b[1;32m--> 249\u001b[0m     multiline_rg \u001b[38;5;241m=\u001b[39m \u001b[43mparse_bfrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiline_rg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m process_char:\n\u001b[0;32m    251\u001b[0m     parse_bfchar(l, map_dict, int_entry)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\PyPDF2\\_cmap.py:304\u001b[0m, in \u001b[0;36mparse_bfrange\u001b[1;34m(l, map_dict, int_entry, multiline_rg)\u001b[0m\n\u001b[0;32m    300\u001b[0m closure_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m a \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m b:\n\u001b[0;32m    302\u001b[0m     map_dict[\n\u001b[0;32m    303\u001b[0m         unhexlify(fmt \u001b[38;5;241m%\u001b[39m a)\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m--> 304\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharmap\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmap_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16-be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    306\u001b[0m         )\n\u001b[0;32m    307\u001b[0m     ] \u001b[38;5;241m=\u001b[39m unhexlify(fmt2 \u001b[38;5;241m%\u001b[39m c)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16-be\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    308\u001b[0m     int_entry\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[0;32m    309\u001b[0m     a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = credit_transactions_to_df(files)\n",
    "\n",
    "debit_transactions = df[df['Credit Amount'].isna()]['Transaction Description'].tolist()\n",
    "\n",
    "debit_categories = [\"Savings\",\"Rent\",\"Eating out\",\"Transport\",\"Groceries\",\"Shopping\",\"Holidays\",\"Entertainment\",\"Personal Care\",\"General\",\"Charity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are going to ask ChatGPT to categorise this transactional data for us\n",
    "* I have noticed that quality of ChatGPT's category assignment decreases with the length of the prompt\n",
    "* To combat this, I recommend splitting the debit_transactions data into manageable chunks\n",
    "* I will fire off multiple prompts to ChatGPT via the API, and concatenate the outputs using pandas\n",
    "* ChatGPT's categorisation is good, but not perfect - I advise manual spot checks to confirm you are happy with the results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define ChatGPRT Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt = \"\"\"\n",
    "I have the following list of transaction descriptions, which I want you to map to a category from the list of categories provided.   \n",
    "Here is the list of categories: {}\n",
    "Here is the list of transaction descriptions: {}\n",
    "I don't want you to provide me the code for doing this. I want you to actually do the mapping, using the data provided.\n",
    "Please give me your response as a dictionary object, in the form: \"Transaction Description\":\"Category\"\n",
    "\"\"\"\n",
    "\n",
    "follow_up_prompt = \"\"\"\n",
    "That's perfect! Can you repeat this task for the next {} transaction descriptions, returning an output with no duplicates?\n",
    "Here are the transaction descriptions: {} \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fire off API requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through 100 transactions at a time, giving first prompt on first round and subsequently follow-up prompts only\n",
    "boolean = True\n",
    "for chunk in chunks(debit_transactions,n=100):\n",
    "    if boolean:\n",
    "        response = query_chatGPT(first_prompt.format(debit_categories, chunk))\n",
    "        \n",
    "        # Captures all text within curly brackets (aka the output dictionary)\n",
    "        credit_card_category_mappings = extract_dict_from_chatGPT_response(response)\n",
    "        \n",
    "        boolean = False\n",
    "        \n",
    "        #print(first_prompt.format(debit_categories, chunk))\n",
    "    else:\n",
    "        response = query_chatGPT(follow_up_prompt.format(len(chunk), chunk))\n",
    "        \n",
    "        # Captures all text within curly brackets (aka the output dictionary)\n",
    "        credit_card_category_mappings.update(extract_dict_from_chatGPT_response(response))\n",
    "\n",
    "#         print(follow_up_prompt.format(len(chunk), chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit_card_category_mappings.json') as json_file:\n",
    "    credit_card_category_mappings = load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge category mappings with transactions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Debit Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charity</td>\n",
       "      <td>59.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eating out</td>\n",
       "      <td>6610.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Entertainment</td>\n",
       "      <td>4057.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>General</td>\n",
       "      <td>2401.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Groceries</td>\n",
       "      <td>7180.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Holidays</td>\n",
       "      <td>7455.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Personal Care</td>\n",
       "      <td>1779.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Rent</td>\n",
       "      <td>380.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>7545.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transport</td>\n",
       "      <td>8278.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category  Debit Amount\n",
       "0        Charity         59.29\n",
       "1     Eating out       6610.37\n",
       "2  Entertainment       4057.67\n",
       "3        General       2401.96\n",
       "4      Groceries       7180.30\n",
       "5       Holidays       7455.97\n",
       "6  Personal Care       1779.51\n",
       "7           Rent        380.00\n",
       "8       Shopping       7545.74\n",
       "9      Transport       8278.81"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure transaction descriptions (keys) are uppercase for dataframe merge\n",
    "transaction_categories = {k.upper():v for k,v in credit_card_category_mappings.items()}\n",
    "\n",
    "# Convert to dictionary to dataframe\n",
    "categories_df = pd.DataFrame(data=transaction_categories.items(),columns=[\"Transaction Description\",\"Category\"],index=range(len(transaction_categories)))\n",
    "\n",
    "# Merge category mappings with all transaction records\n",
    "credit_card_df = pd.merge(df, categories_df, on='Transaction Description', how='left')\n",
    "\n",
    "# Find out what % of money spent using Credit Card has been categorised, against credit card repayments on debit card \n",
    "credit_card_df.groupby(['Category'])['Debit Amount'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Spend on Credit Card Statements Processed: £45749.62\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Spend on Credit Card Statements Processed: £{credit_card_df['Debit Amount'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recategorise Null Categories from Credit Card Statements via Chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncategorised = credit_card_df[(credit_card_df['Category'].isna())&(credit_card_df['Transaction Description']!='DIRECT DEBIT PAYMENT - THANK YOU')]\n",
    "if len(uncategorised) > 0:\n",
    "    print(uncategorised['Transaction Description'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning: Remove Direct Debit Credit Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = credit_card_df[~credit_card_df['Transaction Description'].str.contains(\"DIRECT DEBIT PAYMENT - THANK YOU|PAYMENT RECEIVED - THANK YOU\",regex=True)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output categorised transactions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r\"finances_categorised.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
